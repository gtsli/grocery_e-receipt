{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Flatten, Embedding, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "import time\n",
    "from random import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../synthetic_data/data/final_train_labels.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [ea[1] for ea in data]\n",
    "y = [ea[0] for ea in data]\n",
    "df = pd.DataFrame(data={\"X\": x, \"Y\": y})\n",
    "print (len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y Vocab\n",
    "labels = y\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['encoded_y'] = le.fit_transform(y)\n",
    "print (\"# categories = \", max(df.encoded_y.tolist())+1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform([8681])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Vocab\n",
    "MAX_SEQ_LEN = 25\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "flattened = np.asarray(x).reshape(-1)\n",
    "t = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "t.fit_on_texts(flattened)\n",
    "\n",
    "## see encoded X --> do batch encoding, cannot load into memory\n",
    "encoded_x = t.texts_to_sequences(flattened)\n",
    "train_x = pad_sequences(encoded_x, maxlen=MAX_SEQ_LEN)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(df.encoded_y.tolist(), num_classes=max(df.encoded_y.tolist())+1)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SEQ_LEN))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_x.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(train_x, train_y, epochs=5)\n",
    "model.save(\"models/lstm_50k_epochs_5.h5\")\n",
    "print (\"Took: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/lstm_50k_epochs_5.h5\")\n",
    "test_num = 300\n",
    "preds = model.predict(train_x[:test_num])\n",
    "pred_labels = [[np.argmax(x)] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_titles = le.inverse_transform(pred_labels)\n",
    "pred_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame({'pred_x': pred_titles, 'x': x[:test_num], 'y': y[:test_num]})\n",
    "analysis_df = analysis_df[['x', 'y', 'pred_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pickled/labelencoder_classes.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(t, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
